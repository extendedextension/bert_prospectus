{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b1ee2d-1def-4f3b-8ce8-7b8ce7b234f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94c57d",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc9bb9",
   "metadata": {},
   "source": [
    "### Extracting prospectuses summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334905ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looad data\n",
    "df = pd.read_csv(\"/Users/annabartyzel/Library/CloudStorage/GoogleDrive-ania.bartyzel45@gmail.com/My Drive/MSc FRM/Term 1/Advanced Topics in Financial Modelling and Technologies/COMP0257 Research Project/Final/prospectuses.csv\")\n",
    "stock_performance_df = pd.read_csv(\"/Users/annabartyzel/Library/CloudStorage/GoogleDrive-ania.bartyzel45@gmail.com/My Drive/MSc FRM/Term 1/Advanced Topics in Financial Modelling and Technologies/COMP0257 Research Project/Final/stock_performance.csv\")\n",
    "\n",
    "# Function to extract the summary from the ProspectusText\n",
    "def extract_summary(text):\n",
    "    try:\n",
    "        match = re.search(r\"This summary highlights(.*?)involves a high degree of risk\", text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        else:\n",
    "            return \"\" \n",
    "    except Exception as e:\n",
    "        return \"\"  \n",
    "\n",
    "# Apply the function to the ProspectusText column\n",
    "df['ProspectusSummary'] = df['ProspectusText'].apply(extract_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5c30c",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cced8",
   "metadata": {},
   "source": [
    "Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9e7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Tickers\n",
    "df['Ticker'] = df['Ticker'].str.strip().str.upper()\n",
    "stock_performance_df['Ticker'] = stock_performance_df['Ticker'].str.strip().str.upper()\n",
    "\n",
    "# Filter rows with missing ProspectusSummary\n",
    "df = df[\n",
    "    df['ProspectusSummary'].notna() & \n",
    "    (df['ProspectusSummary'].str.strip() != \"\")\n",
    "]\n",
    "\n",
    "# Filter prospectuses for only companies with available stock performance data\n",
    "valid_tickers = set(stock_performance_df['Ticker'])\n",
    "df = df[df['Ticker'].isin(valid_tickers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ee7d",
   "metadata": {},
   "source": [
    "Cleaning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c745febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_prospectuses(text):\n",
    "    text = re.sub(r'https://www\\.sec\\.gov/\\S+', '', text)  # Remove SEC links\n",
    "    text = re.sub(r'\\btable of contents\\b', '', text, flags=re.IGNORECASE)  # Remove \"Table of Contents\"\n",
    "    text = re.sub(r'\\b\\d{2}:\\d{2}\\b', '', text)  # Remove time stamps\n",
    "    text = re.sub(r'\\b26/12/2024\\b', '', text)  # Remove date stamp \"26/12/2024\"\n",
    "    text = re.sub(r'\\b\\d+/\\d+\\b', '', text)  # Remove page numbers as fractions\n",
    "    return text \n",
    "    \n",
    "# Apply preprocessing to the ProspectusText column\n",
    "df[\"ProcessedSummary\"] = df[\"ProspectusSummary\"].apply(preprocess_prospectuses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634aa7d7",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af17a42",
   "metadata": {},
   "source": [
    "### Tokenizing and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9307ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\") # Cased version, in consideration for names e.g. Apple vs apple etc.\n",
    "\n",
    "# Function to tokenize and chunk \n",
    "def tokenize_and_chunk(text, max_length=510, overlap=50):\n",
    "    tokens = tokenizer.tokenize(text)  # Tokenize the full text using WordPiece tokenizer\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        # Chunking \n",
    "        end = start + max_length\n",
    "        chunk_tokens = tokens[start:start + max_length -2]  # Leave space for [CLS] and [SEP] tokens\n",
    "\n",
    "        # Manually add special tokens\n",
    "        chunk_tokens = [tokenizer.cls_token] + chunk_tokens + [tokenizer.sep_token]\n",
    "\n",
    "        # Convert tokens to input IDs\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(chunk_tokens)\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Ensure consistent padding to 512 tokens\n",
    "        padding_length = 512 - len(input_ids)\n",
    "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "\n",
    "        # Append the chunk\n",
    "        chunks.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        })\n",
    "\n",
    "        # Move to the next chunk with overlap\n",
    "        start += max_length - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Apply tokenization and chunking to the ProcessedSummary column\n",
    "df[\"BERT_Chunks\"] = df[\"ProcessedSummary\"].apply(tokenize_and_chunk)\n",
    "\n",
    "df = df.explode(\"BERT_Chunks\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef882e",
   "metadata": {},
   "source": [
    "# Generating BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ffc04",
   "metadata": {},
   "source": [
    "### BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d810d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to compute embeddings for a single chunk\n",
    "def compute_embedding(chunk):\n",
    "\n",
    "    # Extract input_ids and attention_mask from the chunk, and convert them into PyToch tensors with batch dimension\n",
    "    input_ids = torch.tensor(chunk[\"input_ids\"]).unsqueeze(0) \n",
    "    attention_mask = torch.tensor(chunk[\"attention_mask\"]).unsqueeze(0) \n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Processes inputs through model's transformer layers\n",
    "    \n",
    "    # Extract the [CLS] token's embedding\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0) \n",
    "    return cls_embedding.numpy()  # Convert to NumPy\n",
    "\n",
    "# Apply embedding function to the BERT_Chunks column\n",
    "df[\"BERT_Embedding\"] = df[\"BERT_Chunks\"].apply(compute_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f39b7",
   "metadata": {},
   "source": [
    "### Average embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bee5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average embeddings for each Ticker\n",
    "df = (\n",
    "    df.groupby(\"Ticker\")[\"BERT_Embedding\"]\n",
    "    .apply(lambda embeddings: np.mean(np.stack(embeddings), axis=0)) \n",
    "    .reset_index() \n",
    ")\n",
    "\n",
    "# Rename the BERT_Embedding column to AverageEmbedding\n",
    "df.rename(columns={\"BERT_Embedding\": \"AverageEmbedding\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2787b8f",
   "metadata": {},
   "source": [
    "### Stock performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a090b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stock performance data with average embeddings for each Ticker\n",
    "stock_performance_df[\"Performance\"] = stock_performance_df[\"Performance\"].str.rstrip('%').astype(float) # Convert stock performance to numeric\n",
    "df = df.merge(\n",
    "    stock_performance_df, \n",
    "    on=\"Ticker\", \n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d48ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ac72a-74d1-4dd5-a9a7-017c81931ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
