### Data
The initial sample consisted of prospectuses from 226 out of 232 IPOs in the US stock market in 2019. The prospectuses were downloaded from SEC’s EDGAR database and daily stock prices taken from Compustat. Using the prices, 1-year post IPO absolute return was derived for each company.  After removing companies with missing stock data and extracting ‘Prospectus Summary’ sections from the full prospectus text only 103 companies were left. The final dataset used for analysis consists of company tickers, corresponding performance data and BERT word embeddings generated from the ‘Prospectus Summary.'

### BERT model
Pre-trained BERT model from Hugging Face was used. The model was pretrained on BookCorpus and English Wikipedia using two unsupervised tasks. Masked Language Modelling (MLM) involved masking random token inputs and predicting them based on the surrounding context. The other task being Next Sentence Prediction (NSP), where BERT learned the relationship between two sentences (Devlin et al., 2019). 
The model’s architecture is a multi-layer bidirectional Transformer encoder, which relies on the self-attention mechanism and allows the model to capture contextual meaning. The mechanism computes attention scores for the tokens in a sequence, with the scores determining how much focus one token should give to another. Moreover, instead of a single attention function, the model performs the attention function in parallel, capturing various aspects of the relationship between tokens (Vaswani et al., 2017). Alongside the attention layers, the encoder consists of feed-forward networks, through which the tokens pass. The model’s architecture consists of 12 encoder layers.
