### Data
The initial sample consisted of prospectuses from 226 out of 232 IPOs in the US stock market in 2019. The prospectuses were downloaded from SEC’s EDGAR database and daily stock prices taken from Compustat. Using the prices, 1-year post IPO absolute return was derived for each company.  After removing companies with missing stock data and extracting ‘Prospectus Summary’ sections from the full prospectus text only 103 companies were left. The final dataset used for analysis consists of company tickers, corresponding performance data and BERT word embeddings generated from the ‘Prospectus Summary.'

### BERT model
Pre-trained BERT model from Hugging Face was used. The model was pretrained on BookCorpus and English Wikipedia using two unsupervised tasks. Masked Language Modelling (MLM) involved masking random token inputs and predicting them based on the surrounding context. The other task being Next Sentence Prediction (NSP), where BERT learned the relationship between two sentences (Devlin et al., 2019). 
The model’s architecture is a multi-layer bidirectional Transformer encoder, which relies on the self-attention mechanism and allows the model to capture contextual meaning. The mechanism computes attention scores for the tokens in a sequence, with the scores determining how much focus one token should give to another. Moreover, instead of a single attention function, the model performs the attention function in parallel, capturing various aspects of the relationship between tokens. Alongside the attention layers, the encoder consists of feed-forward networks, through which the tokens pass. The model’s architecture consists of 12 encoder layers.


### Tokens and embeddings 
The ‘Prospectus Summary’ text is tokenized using WordPiece which has 30,000 token vocabulary (Wu et al., 2016). To adhere to BERT’s 512 tokens limit, the token sequences are chunked. Each sequence begins with a special classification token ([CLS]), with the final hidden state of this token serving as the aggregate sequence representation. Special separation token ([SEP]) is also added to separate sequences. After tokenizing, the BERT tokenizer converts the tokens into input IDs, assigning an integer ID based on its vocabulary to each token. Padding tokens are included to maintain consistency, so all sequences have the same length, facilitating batch processing. Additionally, an attention mask is generated enabling the model to ignore padding tokens during the self-attention mechanism. An overlap of 50 tokens is maintained, to ensure no loss of meaning between chunks.


For each token, the BERT model generates an input representation by summing its corresponding token, segment and position embeddings.  This captures the token’s identity, role within the segment and position in the sequence. After processing through its layers, the model generates hidden states for each token with the final embeddings effectively capturing the token’s contextual meaning. [CLS] are extracted from the hidden state as it represents the entire sequence in a single vector. 

![BERT input representation Source: Devlin et al. (2019)](bert.img)

After obtaining [CLS] embeddings to consolidate the chunks, the embeddings were averaged for each company to create a single embedding per firm. These were then merged with stock performance data based on company Ticker values. For more details, refer to Vaswani et al. (2017).
